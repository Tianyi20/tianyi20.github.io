---
title: 'RNN-GRU'
date: 2024-12-2
permalink: /posts/2024/12/RNN-GRU/
tags:
  - RNN-GRU

---
![image](https://github.com/user-attachments/assets/07c61aa0-d203-4a7e-af6e-cafc0446e094)

RNN用于处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。理论上，RNN能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关。

**RNN重要特点：每一步的参数共享**


首先给出RNN数学模型

![image](https://github.com/user-attachments/assets/7da9b351-9a3b-43e1-a199-bef079adc90a)

这也是最重要的，每一个当前层的参数和上一个层的参数有关，这就实现了实现当前输入结果与之前的计算挂钩的目的。

这里分开来看就是这样的
![image](https://github.com/user-attachments/assets/c66595d3-7012-4964-addf-5b3b8035ee15)

![image](https://github.com/user-attachments/assets/450f1fed-4a3a-42de-bf70-cc5332398982)


## GRU 引入了门机制
![image](https://github.com/user-attachments/assets/dedfdb55-9345-4143-afe0-28a7393eef36)
![image](https://github.com/user-attachments/assets/0106d4cc-8521-4984-bb6e-e0549a9006d3)


The Gated Recurrent Unit (GRU) addresses these limitations by introducing **gate mechanisms** that control the flow of information, allowing the network to focus on relevant long-term dependencies while forgetting irrelevant details. Its update equations are as follows:

1. **Update Gate**:
   
   $$
   z_t = \sigma(W_zx_t + U_zh_{t-1} + b_z)
   $$
   
   Determines how much of the previous hidden state $ h_{t-1} $ should be carried forward to the current hidden state $ h_t $.

3. **Reset Gate**:
   
   $$ r_t = \sigma(W_rx_t + U_rh_{t-1} + b_r) $$
   
   Controls how much of the previous state information should be forgotten when calculating the candidate hidden state \( \tilde{h}_t \).

5. **Candidate Hidden State**:

   $$
   \tilde{h}_t = \tanh(W_hx_t + U_h(r_t \odot h_{t-1}) + b_h)
   $$

   A preliminary hidden state, modulated by the reset gate.

7. **Final Hidden State**:


   $$
   h_t = z_t \odot \tilde{h}_t + (1 - z_t) \odot h_{t-1}
   $$

   Combines the candidate hidden state and the previous hidden state using the update gate.

---

#### **4. Key Differences**
| **Feature**              | **Vanilla RNN**                         | **GRU**                                   |
|--------------------------|-----------------------------------------|------------------------------------------|
| **Architecture**          | Single hidden state, no gates          | Hidden state with update and reset gates |
| **Gradient Handling**     | Prone to vanishing/exploding gradients  | Mitigates gradient issues via gates      |
| **Memory Management**     | Poor long-term memory                  | Efficient handling of long-term memory   |
| **Complexity**            | Simple                                 | Moderate (less than LSTM)                |
| **Long-Term Dependencies**| Weak                                   | Strong                                   |

---
